# ChessFM ğŸ§ â™Ÿï¸

> *A 1.5B parameter model that plays chess by reasoning, not memorizing.*

[![Status](https://img.shields.io/badge/Status-Research-orange)]()
[![Model](https://img.shields.io/badge/Base-Qwen--2.5--Math--1.5B-blue)]()
[![Target](https://img.shields.io/badge/Target-1200%20Elo-green)]()

---

## ğŸ’¡ The Idea

Most chess bots play by brute-force search. ChessFM plays by **thinking out loud**:

```xml
<think>
The opponent's queen threatens my f7 pawn.
If I castle now, I lose material.
Better to block with Nf6 first.
</think>
Nf6
```

The model explains *why* it's making a move â€” like a chess tutor, not a calculator.

---

## ğŸ¯ Goals

| Metric | Target |
|--------|--------|
| **Elo Rating** | 1200+ (beat most LLMs) |
| **Illegal Move Rate** | < 5% |
| **Reasoning Format** | > 95% valid `<think>` tags |

### Benchmarks

| Model | Elo |
|-------|-----|
| GPT-4o | ~1050 |
| Gemini Pro | ~1050 |
| Claude Sonnet | ~1000 |
| **ChessFM (target)** | **1200** |

---

## ğŸ”¬ Approach

### Phase 1: Direct GRPO (Reinforcement Learning)
Train directly on chess games using verifiable rewards (legal/illegal, win/lose).
No synthetic data needed â€” Stockfish provides the reward signal.

### Phase 2: SFT Enhancement (Optional)
Add reasoning traces using the [data generation scripts](data_generation/).
Use free AI credits from Antigravity, Kiro, or Qwen to generate `<think>` reasoning.

### Phase 3: Curriculum Learning
Progressive difficulty: Random â†’ Stockfish L1 â†’ Stockfish L3

---

## ğŸ› ï¸ Stack

| Component | Tool | Purpose |
|-----------|------|---------|
| **Base Model** | [Qwen-2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B-Instruct) | Better reasoning pre-training |
| **Training** | [unsloth](https://github.com/unslothai/unsloth) | 2x faster, 60% less VRAM |
| **Inference** | [vLLM](https://github.com/vllm-project/vllm) | Fast game rollouts |
| **Chess Engine** | [Stockfish 16](https://stockfishchess.org/) | Reward signal + validation |
| **Hardware** | RTX 4090 (RunPod) | 24GB VRAM |

---

## ğŸ“Š Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   FEN Position                                              â”‚
â”‚        â†“                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Direct     â”‚ â†’   â”‚   GRPO vs   â”‚ â†’   â”‚   Elo       â”‚  â”‚
â”‚   â”‚  GRPO on    â”‚     â”‚  Stockfish  â”‚     â”‚   Eval      â”‚  â”‚
â”‚   â”‚  legal/win  â”‚     â”‚  curriculum â”‚     â”‚   (500      â”‚  â”‚
â”‚   â”‚  rewards    â”‚     â”‚             â”‚     â”‚   games)    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†‘                                                   â”‚
â”‚   (Optional: SFT with data_generation/ for <think> traces) â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Bonus Features (After v1)

- **Socratic Structure** â€” Force reasoning into `<threat_scan>`, `<candidates>`, `<verification>` tags
- **Negative Data** â€” Train on mistake-then-correction examples
- **Puzzle Training** â€” Tactical curriculum from Lichess puzzles

---

## ï¿½ Cost Estimate

| Phase | Time | Cost |
|-------|------|------|
| Setup & Baseline | 4 hr | $1.80 |
| SFT Training | 4 hr | $1.80 |
| GRPO Training | 20 hr | $9.00 |
| **Total** | **~28 hr** | **~$13** |

*Yes, you can train a chess-playing LLM for the price of lunch.*

---

## ğŸ“š References

- [GRPO Paper](https://arxiv.org/abs/2402.03300) â€” Our RL algorithm
- [Qwen2.5-Math](https://arxiv.org/abs/2409.12122) â€” Base model
- [DeepSeek-R1](https://arxiv.org/abs/2501.12948) â€” Self-correction patterns
- [Dynomight Chess](https://dynomight.substack.com/p/chess) â€” Regurgitation technique

---

## ğŸ“‹ Roadmap

See the full [ChessFM Roadmap](chess_fm_roadmap.md) for detailed implementation steps.

---

## ğŸ—‚ï¸ Project Structure

```
chess-fm/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ chess_fm_roadmap.md       # Detailed implementation plan
â””â”€â”€ data_generation/          # SFT data generation scripts
    â”œâ”€â”€ generate_sft_data_proxy.py  # Use Antigravity/Kiro/Qwen
    â””â”€â”€ README.md                   # Setup instructions
```

---

## ğŸ“„ License

MIT
