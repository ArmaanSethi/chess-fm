# ChessFM ğŸ§ â™Ÿï¸

> *A 1.5B parameter model that plays chess by reasoning, not memorizing.*

[![Status](https://img.shields.io/badge/Status-Research-orange)]()
[![Model](https://img.shields.io/badge/Base-Qwen--2.5--3B-blue)]()
[![Target](https://img.shields.io/badge/Target-1200%20Elo-green)]()

---

## ğŸ’¡ The Idea

Most chess bots play by brute-force search. ChessFM plays by **thinking out loud**:

```xml
<think>
The opponent's queen threatens my f7 pawn.
If I castle now, I lose material.
Better to block with Nf6 first.
</think>
Nf6
```

The model explains *why* it's making a move â€” like a chess tutor, not a calculator.

---

## ğŸ¯ Goals

| Metric | Target |
|--------|--------|
| **Elo Rating** | 1200+ (beat most LLMs) |
| **Illegal Move Rate** | < 5% |
| **Reasoning Format** | > 95% valid `<think>` tags |

### Benchmarks

| Model | Elo |
|-------|-----|
| GPT-4o | ~1050 |
| Gemini Pro | ~1050 |
| Claude Sonnet | ~1000 |
| **ChessFM (target)** | **1200** |

---

## ğŸ”¬ Approach

### Phase 1: SFT Bootstrap
Train on reasoning traces to teach the model chess fundamentals and `<think>` format.

### Phase 2: Direct GRPO (Reinforcement Learning)
Train directly on chess games using verifiable rewards (legal/illegal, win/lose).
Stockfish provides the reward signal for curriculum learning.

### Phase 3: Curriculum Learning
Progressive difficulty: Random â†’ Stockfish L1 â†’ Stockfish L3

---

## ğŸ› ï¸ Stack

| Component | Tool | Purpose |
|-----------|------|---------|
| **Base Model** | [Qwen-2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) | Best format adherence in benchmarks |
| **Training** | [unsloth](https://github.com/unslothai/unsloth) | 2x faster, 60% less VRAM |
| **Inference** | [vLLM](https://github.com/vllm-project/vllm) | Fast game rollouts |
| **Chess Engine** | [Stockfish 16](https://stockfishchess.org/) | Reward signal + validation |
| **Hardware** | RTX 4090 (RunPod) | 24GB VRAM |

---

## ğŸ“Š Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   FEN Position                                              â”‚
â”‚        â†“                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  SFT on     â”‚ â†’   â”‚   GRPO vs   â”‚ â†’   â”‚   Elo       â”‚  â”‚
â”‚   â”‚  reasoning  â”‚     â”‚  Stockfish  â”‚     â”‚   Eval      â”‚  â”‚
â”‚   â”‚  traces     â”‚     â”‚  curriculum â”‚     â”‚   (500      â”‚  â”‚
â”‚   â”‚  (185 smpl) â”‚     â”‚             â”‚     â”‚   games)    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Bonus Features (After v1)

- **Socratic Structure** â€” Force reasoning into `<threat_scan>`, `<candidates>`, `<verification>` tags
- **Negative Data** â€” Train on mistake-then-correction examples
- **Puzzle Training** â€” Tactical curriculum from Lichess puzzles

---

## ğŸ’° Cost Estimate

| Phase | Time | Cost |
|-------|------|------|
| Setup & Baseline | 4 hr | $1.80 |
| SFT Training | 4 hr | $1.80 |
| GRPO Training | 20 hr | $9.00 |
| **Total** | **~28 hr** | **~$13** |

*Yes, you can train a chess-playing LLM for the price of lunch.*

---

## ğŸ“š References

- [GRPO Paper](https://arxiv.org/abs/2402.03300) â€” Our RL algorithm
- [Qwen2.5-Math](https://arxiv.org/abs/2409.12122) â€” Base model architecture
- [DeepSeek-R1](https://arxiv.org/abs/2501.12948) â€” Self-correction patterns
- [Dynomight Chess](https://dynomight.substack.com/p/chess) â€” Regurgitation technique

---

## ğŸ“‹ Roadmap

See the full [ChessFM Roadmap](chess_fm_roadmap.md) for detailed implementation steps.

---

## ğŸ—‚ï¸ Project Structure

```
chess-fm/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ chess_fm_roadmap.md       # Detailed implementation plan
â”œâ”€â”€ requirements.txt          # All dependencies
â”œâ”€â”€ setup_env.sh              # Environment setup script
â”‚
â”œâ”€â”€ data_generation/          # SFT data generation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ fetch_elite_data.py   # Fetch FENs from Lichess
â”‚   â”œâ”€â”€ download_positions.py # Generate diverse positions
â”‚   â”œâ”€â”€ convert_to_training.py
â”‚   â”œâ”€â”€ positions.txt         # 25k elite FENs
â”‚   â””â”€â”€ all_sft_data.jsonl    # 185 deduplicated samples
â”‚
â”œâ”€â”€ training/                 # SFT training
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ train_sft.py
â”‚
â”œâ”€â”€ rl/                       # Reinforcement learning
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ chess_env.py          # Chess environment
â”‚   â”œâ”€â”€ rewards.py            # Reward functions
â”‚   â”œâ”€â”€ train_grpo.py         # GRPO training
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ benchmarks/               # Evaluation & baselines
â”‚   â””â”€â”€ phase0/
â”‚       â”œâ”€â”€ BASELINE_REPORT.md
â”‚       â”œâ”€â”€ STRATEGY.md
â”‚       â”œâ”€â”€ benchmark_models.py
â”‚       â””â”€â”€ run_benchmark_mlx.py
â”‚
â”œâ”€â”€ scripts/                  # Utilities
â”‚   â”œâ”€â”€ audit_tokenizer.py
â”‚   â””â”€â”€ download_models.py
â”‚
â””â”€â”€ tests/                    # Unit tests
    â””â”€â”€ test_data_generation.py
```

---

## ğŸ“„ License

MIT
